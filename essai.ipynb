{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des modules librairires et modules nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création premier df valeurs numériques activités utilisateurs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test avec Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de jours dans le mois de janvier\n",
    "num_days_in_january = 31\n",
    "num_days = 30000\n",
    "total_records = num_days_in_january * num_days\n",
    "\n",
    "# Générer les dates pour chaque jour dU MOIS DE Janvier 2024\n",
    "start_date = datetime.strptime('2024-01-01', '%Y-%m-%d')\n",
    "dates = np.array([start_date + timedelta(days=i // num_days) for i in range(total_records)])\n",
    "\n",
    "# Générer les IDs utilisateurs de 1 à 30 000 et les répéter pour chaque jour\n",
    "user_ids = np.tile(np.arange(1, num_days + 1), num_days_in_january)\n",
    "\n",
    "# Génération des données basées sur une distribution normale pour ne pas avoir de valeurs étranges\n",
    "data = {\n",
    "    'User_id': pl.Series('User_id', range(1, num_days + 1)),\n",
    "    'Day': [date.strftime('%d-%b') for date in dates],\n",
    "    'Calories Burned': pl.Series('Calories Burned', np.clip(np.random.normal(2200, 200, size=num_days), 0, None)),\n",
    "    'Steps': pl.Series('Steps', np.clip(np.random.normal(15000, 2000, size=num_days), 0, None)),\n",
    "    'Distance': pl.Series('Distance', np.clip(np.random.normal(5, 1, size=num_days), 0, None)),\n",
    "    'Minutes Sedentary': pl.Series('Minutes Sedentary', np.clip(np.random.normal(600, 100, size=num_days), 0, None)),\n",
    "    'Minutes Lightly Active': pl.Series('Minutes Lightly Active', np.clip(np.random.normal(200, 50, size=num_days), 0, None)),\n",
    "    'Minutes Fairly Active': pl.Series('Minutes Fairly Active', np.clip(np.random.normal(20, 10, size=num_days), 0, None)),\n",
    "    'Minutes Very Active': pl.Series('Minutes Very Active', np.clip(np.random.normal(20, 10, size=num_days), 0, None)),\n",
    "    'Activity Calories': pl.Series('Activity Calories', np.clip(np.random.normal(1000, 200, size=num_days), 0, None)),\n",
    "    'Minutes Asleep': pl.Series('Minutes Asleep', np.clip(np.random.normal(350, 50, size=num_days), 0, None)),\n",
    "    'Minutes Awake': pl.Series('Minutes Awake', np.clip(np.random.normal(60, 20, size=num_days), 0, None)),\n",
    "    'Number of Awakenings': pl.Series('Number of Awakenings', np.clip(np.random.normal(10, 5, size=num_days), 0, None)),\n",
    "    'Time in Bed': pl.Series('Time in Bed', np.clip(np.random.normal(450, 60, size=num_days), 0, None)),\n",
    "    'Minutes REM Sleep': pl.Series('Minutes REM Sleep', np.clip(np.random.normal(100, 20, size=num_days), 0, None)),\n",
    "    'Minutes Light Sleep': pl.Series('Minutes Light Sleep', np.clip(np.random.normal(250, 50, size=num_days), 0, None)),\n",
    "    'Minutes Deep Sleep': pl.Series('Minutes Deep Sleep', np.clip(np.random.normal(100, 20, size=num_days), 0, None))\n",
    "}\n",
    "\n",
    "# Création du DataFrame Polars\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schéma du df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats descriptives du dataframe 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>User_id</th><th>Day</th><th>Calories Burned</th><th>Steps</th><th>Distance</th><th>Minutes Sedentary</th><th>Minutes Lightly Active</th><th>Minutes Fairly Active</th><th>Minutes Very Active</th><th>Activity Calories</th><th>Minutes Asleep</th><th>Minutes Awake</th><th>Number of Awakenings</th><th>Time in Bed</th><th>Minutes REM Sleep</th><th>Minutes Light Sleep</th><th>Minutes Deep Sleep</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>500000.0</td><td>&quot;500000&quot;</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td><td>500000.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>250000.5</td><td>null</td><td>2200.513148</td><td>14998.480115</td><td>5.000883</td><td>599.883237</td><td>200.027515</td><td>19.990853</td><td>20.023057</td><td>1000.323278</td><td>349.998706</td><td>60.019923</td><td>10.004967</td><td>450.005684</td><td>100.040819</td><td>250.160094</td><td>99.988459</td></tr><tr><td>&quot;std&quot;</td><td>144337.711635</td><td>null</td><td>200.179112</td><td>1999.237494</td><td>1.000653</td><td>99.967826</td><td>50.017084</td><td>10.023701</td><td>9.997221</td><td>199.934353</td><td>49.994795</td><td>19.992583</td><td>4.993519</td><td>59.918122</td><td>20.007074</td><td>50.022069</td><td>20.021512</td></tr><tr><td>&quot;min&quot;</td><td>1.0</td><td>&quot;01-Apr&quot;</td><td>1298.377887</td><td>5707.849131</td><td>0.52332</td><td>111.381843</td><td>-39.647331</td><td>-23.550161</td><td>-27.036895</td><td>48.623362</td><td>100.750624</td><td>-30.889115</td><td>-12.696687</td><td>162.55034</td><td>3.614543</td><td>18.48783</td><td>12.670889</td></tr><tr><td>&quot;25%&quot;</td><td>125001.0</td><td>null</td><td>2065.535302</td><td>13645.732766</td><td>4.327227</td><td>532.343706</td><td>166.374198</td><td>13.242142</td><td>13.286002</td><td>865.633054</td><td>316.272751</td><td>46.558684</td><td>6.629895</td><td>409.375881</td><td>86.591684</td><td>216.45313</td><td>86.513007</td></tr><tr><td>&quot;50%&quot;</td><td>250001.0</td><td>null</td><td>2200.166657</td><td>14998.209257</td><td>5.002487</td><td>599.709119</td><td>200.057179</td><td>20.009027</td><td>20.012442</td><td>1000.22872</td><td>349.900735</td><td>60.032936</td><td>10.000324</td><td>450.009084</td><td>100.034639</td><td>250.138529</td><td>99.980293</td></tr><tr><td>&quot;75%&quot;</td><td>375000.0</td><td>null</td><td>2335.341914</td><td>16349.40194</td><td>5.676382</td><td>667.313589</td><td>233.746926</td><td>26.747538</td><td>26.760287</td><td>1135.068164</td><td>383.77443</td><td>73.502136</td><td>13.368244</td><td>490.611867</td><td>113.523477</td><td>283.851283</td><td>113.499911</td></tr><tr><td>&quot;max&quot;</td><td>500000.0</td><td>&quot;31-Oct&quot;</td><td>3156.982954</td><td>24157.882953</td><td>9.742329</td><td>1069.701294</td><td>454.232828</td><td>68.930607</td><td>65.026121</td><td>1914.083443</td><td>614.7853</td><td>147.577114</td><td>34.46387</td><td>764.922799</td><td>193.221733</td><td>472.024118</td><td>193.892472</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 18)\n",
       "┌────────────┬────────────┬────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ User_id    ┆ Day    ┆ Calories  ┆ … ┆ Time in   ┆ Minutes   ┆ Minutes   ┆ Minutes   │\n",
       "│ ---        ┆ ---        ┆ ---    ┆ Burned    ┆   ┆ Bed       ┆ REM Sleep ┆ Light     ┆ Deep      │\n",
       "│ str        ┆ f64        ┆ str    ┆ ---       ┆   ┆ ---       ┆ ---       ┆ Sleep     ┆ Sleep     │\n",
       "│            ┆            ┆        ┆ f64       ┆   ┆ f64       ┆ f64       ┆ ---       ┆ ---       │\n",
       "│            ┆            ┆        ┆           ┆   ┆           ┆           ┆ f64       ┆ f64       │\n",
       "╞════════════╪════════════╪════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 500000.0   ┆ 500000 ┆ 500000.0  ┆ … ┆ 500000.0  ┆ 500000.0  ┆ 500000.0  ┆ 500000.0  │\n",
       "│ null_count ┆ 0.0        ┆ 0      ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ mean       ┆ 250000.5   ┆ null   ┆ 2200.5131 ┆ … ┆ 450.00568 ┆ 100.04081 ┆ 250.16009 ┆ 99.988459 │\n",
       "│            ┆            ┆        ┆ 48        ┆   ┆ 4         ┆ 9         ┆ 4         ┆           │\n",
       "│ std        ┆ 144337.711 ┆ null   ┆ 200.17911 ┆ … ┆ 59.918122 ┆ 20.007074 ┆ 50.022069 ┆ 20.021512 │\n",
       "│            ┆ 635        ┆        ┆ 2         ┆   ┆           ┆           ┆           ┆           │\n",
       "│ min        ┆ 1.0        ┆ 01-Apr ┆ 1298.3778 ┆ … ┆ 162.55034 ┆ 3.614543  ┆ 18.48783  ┆ 12.670889 │\n",
       "│            ┆            ┆        ┆ 87        ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 25%        ┆ 125001.0   ┆ null   ┆ 2065.5353 ┆ … ┆ 409.37588 ┆ 86.591684 ┆ 216.45313 ┆ 86.513007 │\n",
       "│            ┆            ┆        ┆ 02        ┆   ┆ 1         ┆           ┆           ┆           │\n",
       "│ 50%        ┆ 250001.0   ┆ null   ┆ 2200.1666 ┆ … ┆ 450.00908 ┆ 100.03463 ┆ 250.13852 ┆ 99.980293 │\n",
       "│            ┆            ┆        ┆ 57        ┆   ┆ 4         ┆ 9         ┆ 9         ┆           │\n",
       "│ 75%        ┆ 375000.0   ┆ null   ┆ 2335.3419 ┆ … ┆ 490.61186 ┆ 113.52347 ┆ 283.85128 ┆ 113.49991 │\n",
       "│            ┆            ┆        ┆ 14        ┆   ┆ 7         ┆ 7         ┆ 3         ┆ 1         │\n",
       "│ max        ┆ 500000.0   ┆ 31-Oct ┆ 3156.9829 ┆ … ┆ 764.92279 ┆ 193.22173 ┆ 472.02411 ┆ 193.89247 │\n",
       "│            ┆            ┆        ┆ 54        ┆   ┆ 9         ┆ 3         ┆ 8         ┆ 2         │\n",
       "└────────────┴────────────┴────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_id',\n",
       " 'Day',\n",
       " 'Calories Burned',\n",
       " 'Steps',\n",
       " 'Distance',\n",
       " 'Minutes Sedentary',\n",
       " 'Minutes Lightly Active',\n",
       " 'Minutes Fairly Active',\n",
       " 'Minutes Very Active',\n",
       " 'Activity Calories',\n",
       " 'Minutes Asleep',\n",
       " 'Minutes Awake',\n",
       " 'Number of Awakenings',\n",
       " 'Time in Bed',\n",
       " 'Minutes REM Sleep',\n",
       " 'Minutes Light Sleep',\n",
       " 'Minutes Deep Sleep']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset textuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test sur gtp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test en indiquant que le modèle à utiliser à été entraîné sur données en FR pour voir performance de la génération de texte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test en faisant traitement par lot, optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# Charger le modèle et le tokenizer pour le FR - meilleure perf \n",
    "model_name = \"asi/gpt-fr-cased-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "humeurs = [\"Heureux\", \"Content\", \"Fatigué\", \"Stressé\", \"Motivé\", \"Enervé\", \"Frustré\", \"Relaxé\", \"Neutre\", \"Paisible\"]\n",
    "\n",
    "# Liste des phrases de départ\n",
    "prompts = [\n",
    "    \"Je me sens, \",\n",
    "    \"Aujourd'hui, j'ai\",\n",
    "    \"Ce matin, je\",\n",
    "    \"Cet après-midi, j'ai\",\n",
    "    \"Ce soir, je me sens\"\n",
    "]\n",
    "\n",
    "# Fonction gen-Text\n",
    "def generate_comment(input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=20,\n",
    "        top_p=0.8,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text   \n",
    "\n",
    "num_comments_per_prompt = 3000 # Nombre de commentaires à générer par phrase de prompts\n",
    "comments = []\n",
    "ids = []\n",
    "humeurs_random = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    for i in range(num_comments_per_prompt):\n",
    "        comment = generate_comment(prompt)\n",
    "        comments.append(comment)\n",
    "        ids.append(len(ids) + 1)  # Générer un ID unique\n",
    "        humeurs_random.append(random.choice(humeurs))\n",
    "\n",
    "#Mttre sour forme de dataframe\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'commentaire': comments,\n",
    "    'humeur' : humeurs_random\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>commentaire</th>\n",
       "      <th>humeur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Je me sens,.... Je me suis sentie... Je sens.....</td>\n",
       "      <td>Frustré</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Je me sens,.... comme une bête, dans un état d...</td>\n",
       "      <td>Neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Je me sens,.... comme un étranger qui, après a...</td>\n",
       "      <td>Stressé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Je me sens,.... un peu faible, mais je me suis...</td>\n",
       "      <td>Fatigué</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Je me sens,.... un peu comme si j'étais dans u...</td>\n",
       "      <td>Relaxé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        commentaire   humeur\n",
       "0   1  Je me sens,.... Je me suis sentie... Je sens.....  Frustré\n",
       "1   2  Je me sens,.... comme une bête, dans un état d...   Neutre\n",
       "2   3  Je me sens,.... comme un étranger qui, après a...  Stressé\n",
       "3   4  Je me sens,.... un peu faible, mais je me suis...  Fatigué\n",
       "4   5  Je me sens,.... un peu comme si j'étais dans u...   Relaxé"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enregistrer sur le combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import random\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Charger le modèle et le tokenizer pour le FR - meilleure perf\n",
    "model_name = \"asi/gpt-fr-cased-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "humeurs = [\"Heureux\", \"Content\", \"Fatigué\", \"Stressé\", \"Motivé\", \"Enervé\", \"Frustré\", \"Relaxé\", \"Neutre\", \"Paisible\"]\n",
    "\n",
    "# Liste des phrases de départ\n",
    "prompts = [\n",
    "    \"Ma journée a été\",\n",
    "    \"Aujourd'hui, j'ai\",\n",
    "    \"Ce matin, je\",\n",
    "    \"Cet après-midi, j'ai\",\n",
    "    \"Ce soir, j'ai\"\n",
    "]\n",
    "\n",
    "# Fonction gen-Text\n",
    "def generate_comments(input_text, num_comments, max_length=50, batch_size=20):\n",
    "    comments_batch = []\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    for _ in range(num_comments // batch_size):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=batch_size,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=20,\n",
    "            top_p=0.8,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        batch_comments = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "        comments_batch.extend(batch_comments)\n",
    "    \n",
    "    # Générer les commentaires restants si num_comments n'est pas un multiple de batch_size\n",
    "    remaining_comments = num_comments % batch_size\n",
    "    if remaining_comments > 0:\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=remaining_comments,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=20,\n",
    "            top_p=0.8,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        remaining_batch_comments = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "        comments_batch.extend(remaining_batch_comments)\n",
    "    \n",
    "    return comments_batch\n",
    "\n",
    "num_comments_per_prompt = 1000  # Nombre de commentaires à générer par phrase de prompt\n",
    "comments = []\n",
    "ids = []\n",
    "humeurs_random = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated_comments = generate_comments(prompt, num_comments_per_prompt)\n",
    "    comments.extend(generated_comments)\n",
    "    ids.extend(range(len(ids) + 1, len(ids) + 1 + num_comments_per_prompt))\n",
    "    humeurs_random.extend(random.choices(humeurs, k=num_comments_per_prompt))\n",
    "\n",
    "# Mettre sous forme de dataframe\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'commentaire': comments,\n",
    "    'humeur': humeurs_random\n",
    "})\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiler les comentaires dans un dataframe unqiue avat le merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('commentaires_concaténés.csv')\n",
    "\n",
    "# Concaténer les deux DataFrames\n",
    "df_combined = pd.concat([df1, df], ignore_index=True)\n",
    "\n",
    "df_combined.head()\n",
    "\n",
    "# Sauvegarder le DataFrame concaténé dans un nouveau fichier CSV si nécessaire\n",
    "df_combined.to_csv('commentaires_concaténés2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant Analyse de sentiments:\n",
    "Pré-traitement:\n",
    "- Nettoyer data (suppr caractères spéciaux, ponctuation etc)\n",
    "- Convertir string en minuscules\n",
    "- Supprimer top mots (+ fréquents et peu nformatif)\n",
    "- Lemmatization ou stemming (réduction des mots à leur forme de base).\n",
    "\n",
    "Vectorisation des commentaires:\n",
    "- TF-IDF \n",
    "- Embeddings : Utilisation modèle de préentraînés World2Vec/Glove/FastText qui va représenter mes données textuelles en vecteur.\n",
    "- Transformers : Utilisation modèle comme BERT, GPT-3 qui capturent données texteulles et les reltions entre les mots dans une phrases en utilisant n-grams.\n",
    "\n",
    "Sélection modèle pour Anayse de sentiment:\n",
    "- Regression logistique\n",
    "- Random Forest\n",
    "OU Transformers et modèle comme BERT ou RoBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Chargement du dataframe merge \n",
    "df = pd.read_csv('') \n",
    "\n",
    "# Fonction de pré-traitment des données textuelles\n",
    "def pretraitement_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Retrait ponctuation des coms \n",
    "    text = text.lower()  # Convertir en minuscule tous les caractères \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])  # Retrait des stopwords pour retrait caractère inutiles\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])  # Lemmatization\n",
    "    return text\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "df['commentaires_cleans'] = df['commentaire'].apply(pretraitement_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation des chaînes de caractères\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['commentaires_cleans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test avec Régression Logistique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Diviser les données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['humeur'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction et évaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST - Sentiment Analysis en utilisant BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Chargement du tokenizer et du modèle BERT / faire test aussi avec RoBERTa\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization des données textuelles\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['comments'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Préparation des données pour BERT\n",
    "train_dataset = df.from_pandas(df_train)\n",
    "test_dataset = df.from_pandas(df_test)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Entraînement du modèle\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
