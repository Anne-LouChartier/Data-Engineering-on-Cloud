Etape 1: Etude des besoins et veille technologique
--> comprendre besoins du clients ()
    But de notre application : 
--> Réaliser veille sur techno frameworks big data (reprendre les différentes recherches sur les frameworks et modèles utilisés (turi-create, Surprise...))

Etape 2: Conception de l'architecture distribuée 
--> Dimensionner les ressources nécessaires
--> Chosisir les technolgies adaptées 
--> Définir architecture résiliente et scalable 

Etape 3: Développement des pipelines 
--> Implémenter l'i,gestion des différentes sources (voir dataset, API externes...)
--> Développer les transformations ETL (traitement valeurs nulles, get_dummies des valeurs catégorielles, ajout de la colonne commentaires...)
--> Mettre en place le traitement batch 

Etape 4: Optimisation et monitoring 
--> Optumiser les performances des pipelines  
--> Mettre en place les indicateurs de monitoring (taux de valeur nulles, KPI de dta quality reprendre TP Data Management)

Etape 5: Automatisation 
--> Containeriser
--> Mettre en place l'intégaration et déploiement continus (CI/CD)
--> Automatiser les différents tests 

Etape 6:
SOUTENAAAAAAAANCE








UTILISATION de Faker() 
--> génération de texte sur thématique ()
--> Revoir si pas possible d'utilisation Llama : 


Utilisation TexGen Transformer 
pip install transformers torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Charger le modèle et le tokenizer
model_name = "gpt2" 
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

input_text = "Ma journée aujourd'hui s'est passée"

# Encodage du texte de départ
input_ids = tokenizer.encode(input_text, return_tensors='pt')

# Génération du texte
output = model.generate(
    input_ids,
    max_length=200,  # Nombre maximum de tokens dans le texte généré
    num_return_sequences=1,  # Nombre de séquences à générer
    no_repeat_ngram_size=2,  # Évite les répétitions de n-grammes
    do_sample=True,  # Activation de l'échantillonnage
    top_k=50,  # Limite les choix à top_k probables
    top_p=0.95,  # Utilise la décote cumulative
    temperature=0.7  # Contrôle la créativité du modèle
)

# Décodage du texte généré
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(generated_text)
